{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import cv2\r\n",
    "from tqdm import tqdm\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from keras.utils.np_utils import to_categorical\r\n",
    "from keras.optimizers import Adam\r\n",
    "from keras.preprocessing.image import ImageDataGenerator\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read train.csv\r\n",
    "df = pd.read_csv(\"../input/aaaaaaaaaa/train.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize labels_list, labels_encode\r\n",
    "labels_list = []\r\n",
    "labels_encode = {}\r\n",
    "count = 0\r\n",
    "for i in range(df.shape[0]):\r\n",
    "    if df[\"label\"][i] not in labels_list:\r\n",
    "        labels_list.append(df[\"label\"][i])\r\n",
    "        labels_encode[df[\"label\"][i]] = count\r\n",
    "        count+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels_list = ['3_24+10', '3_24+30', '3_24+5', '3_24+60', '3_24+70', '3_24+90', '3_24+110', '3_24+20', '3_24+40', '3_24+50', '3_24+80', '1_12_1', '1_12_2', '1_13', '1_14', '1_19', '1_24', '1_26', '1_27', '3_21', '3_31', '3_33', '4_4_1', '4_4_2', '4_5_2', '4_5_4', '4_5_5', '4_8_5', '4_8_6', '5_17', '6_2+50', '6_2+70', '6_2+30', '6_2+40', '6_2+60', '6_2+80', '6_7', '7_1', '7_11', '7_13', '7_14', '7_2', '7_4', '7_7', '7_9', 'smoke', 'unknown', '1_11_1', '1_11_2', '1_15', '1_16', '1_18', '1_20_1', '1_22', '1_25', '1_28', '1_29', '1_30', '1_8', '2_3_1', '2_3_L', '2_3_R', '2_6', '2_7', '3_15', '3_17', '3_20', '3_25+70', '3_25+20', '3_25+30', '3_25+40', '3_25+50', '3_25+5', '3_25+60', '3_6', '4_1_6', '4_2_1', '4_2_2', '5_15_5', '6_3_1', '7_3', '7_6', '1_17', '3_16', '5_15_3', '5_20', '7_12', '1_31', '3_10', '3_19', '3_2', '3_5', '3_7', '3_9', '4_1_2_1', '4_1_3_1', '4_5_1', '4_5_6', '4_8_1', '4_8_2', '4_8_3', '5_1', '5_11_1', '5_12_1', '5_13_1', '5_13_2', '5_14_1', '5_14_2', '5_14_3', '5_2', '5_23_2', '5_24_2', '5_3', '5_4', '5_8', '7_5', '3_32', '7_18', '1_2', '1_33', '1_7', '2_4', '3_18_1', '3_18_2', '3_8', '4_1_2', '4_1_3', '5_14', '6_15_2', '6_15_3', '6_6', '6_8_1', '1_1', '1_20_2', '1_20_3', '1_21', '1_23', '1_5', '2_1', '2_2', '2_5', '3_1', '3_26', '3_27', '3_28', '3_29', '3_30', '4_1_1', '4_1_4', '4_1_5', '4_2_3', '4_3', '4_8_4', '5_16', '5_18', '5_19', '5_21', '5_22', '5_5', '5_6', '5_7_1', '5_7_2', '5_9', '6_15_1', '6_16', '6_4', '6_8_2', '6_8_3', '5_29', '5_31+10', '5_31+20', '5_31+30', '5_31+40', '5_31+5', '5_31+50', '5_32', '5_33', '1_6', '5_15_2+2', '5_15_2+1', '5_15_2+3', '5_15_2+5']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load all preprocessed images\r\n",
    "images = []\r\n",
    "root_dir = \"../input/encoded-imgs/Autoencoder/\"\r\n",
    "labels = []\r\n",
    "X = []\r\n",
    "\r\n",
    "for i in tqdm(range(10000)):\r\n",
    "    img_path = root_dir + df[\"filename\"][i][13:]\r\n",
    "    X.append(cv2.resize(cv2.imread(img_path),dsize=(80,80)))\r\n",
    "    labels.append(labels_encode[df[\"label\"][i]])\r\n",
    "    \r\n",
    "X = np.array(X)\r\n",
    "y = np.array(labels)\r\n",
    "Y = to_categorical(y, num_classes=182)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2,stratify = Y, shuffle = True, random_state = 42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aug = ImageDataGenerator(brightness_range=[0.2, 1.8],\r\n",
    "                         rotation_range=10, width_shift_range=0.1,\r\n",
    "                         height_shift_range=0.1, shear_range=0.15, \r\n",
    "                         zoom_range=0.15,\r\n",
    "                         rescale=1/255.0,\r\n",
    "                         fill_mode=\"nearest\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scheduler(epoch):\r\n",
    "    if epoch<= WARMUP_EPOCH:\r\n",
    "        lr = INITIAL_LEARNINGRATE * epoch/WARMUP_EPOCH\r\n",
    "    \r\n",
    "    else:\r\n",
    "        lr = INITIAL_LEARNINGRATE * DECAY_RATE**(epoch-WARMUP_EPOCH)\r\n",
    "    \r\n",
    "    return lr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def BuildModel(ModelName):\r\n",
    "    print('--------------Building The Model...--------------')\r\n",
    "    base_model = tf.keras.applications.InceptionResNetV2(include_top=False,\r\n",
    "                                                         weights='imagenet',\r\n",
    "                                                         input_shape=(80,80,3))\r\n",
    "    base_model.trainable = True\r\n",
    "    print(\"\\nNumber of layers in the base model: \", len(base_model.layers))\r\n",
    "    x = base_model.output\r\n",
    "    #x = tf.keras.layers.GlobalMaxPooling2D()(x)\r\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n",
    "    x = tf.keras.layers.Dense(1024,activation='relu')(x)\r\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\r\n",
    "    out = tf.keras.layers.Dense(182, activation='softmax')(x)\r\n",
    "    model = tf.keras.models.Model(inputs=base_model.input, outputs=out)\r\n",
    "\r\n",
    "    print('\\n--------------Done!--------------')\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ModelDeployment(model, LearningRate, ModelName,  NbOfEpoch, StepsPerEpoch, ValidationSteps):\r\n",
    "    ## Freeze all the layers before the `fine_tune_at` layer\r\n",
    "    #for layer in base_model.layers[:fine_tune_at]:\r\n",
    "    #    layer.trainable =  False\r\n",
    "    print('--------------Deploying the Model...--------------')\r\n",
    "    model.compile(loss = 'categorical_crossentropy', \r\n",
    "                  optimizer=Adam(lr=LearningRate),\r\n",
    "                  metrics='accuracy')\r\n",
    "    monitor = tf.keras.callbacks.EarlyStopping(monitor='loss', \r\n",
    "                                               min_delta=0.0001, \r\n",
    "                                               patience=5, \r\n",
    "                                               verbose=1, \r\n",
    "                                               mode='min',\r\n",
    "                                               restore_best_weights=True)\r\n",
    "    \r\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',\r\n",
    "                                                        factor=0.5,\r\n",
    "                                                        patience=2,\r\n",
    "                                                        verbose=1,\r\n",
    "                                                        mode='min',\r\n",
    "                                                        min_delta=0.0001,\r\n",
    "                                                        cooldown=0,\r\n",
    "                                                        min_lr=MIN_LR,)\r\n",
    "\r\n",
    "    filepath = ModelName + \"-{epoch:02d}.hdf5\"\r\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, \r\n",
    "                                                    monitor='loss', \r\n",
    "                                                    verbose=1, \r\n",
    "                                                    save_best_only=True, \r\n",
    "                                                    save_weights_only=True, \r\n",
    "                                                    mode='min',\r\n",
    "                                                    save_freq = 'epoch') \r\n",
    "    print('--------------Deployed Successfully--------------')\r\n",
    "    print('--------------Training Begins--------------')\r\n",
    "    \r\n",
    "    history = model.fit(aug.flow(X_train, y_train, batch_size = 128), \r\n",
    "                            epochs = NbOfEpoch, \r\n",
    "                            steps_per_epoch = StepsPerEpoch, \r\n",
    "                            validation_data = (X_val,y_val), \r\n",
    "                            verbose = 1, \r\n",
    "                            validation_steps = ValidationSteps,\r\n",
    "                            callbacks = [monitor, checkpoint, lr_scheduler]) #callbacks = [monitor,lr_scheduler]\r\n",
    "    return history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def HistoryPlotting(history):\r\n",
    "    fig,ax = plt.subplots(figsize=(8,5))\r\n",
    "    ax.plot(history.history['loss'],label='train',color='blue')\r\n",
    "    ax.plot(history.history['val_loss'],label='val',color='green')\r\n",
    "    ax.set_title('model loss',fontsize=14)\r\n",
    "    ax.set_ylabel('loss')\r\n",
    "    ax.set_xlabel('epoch')\r\n",
    "    ax.legend(loc='upper right',fontsize=13)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.keras.backend.clear_session()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "WIDTH = 80\r\n",
    "BATCH_SIZE = 128\r\n",
    "model_name = \"InceptionResNetV2\"\r\n",
    "\r\n",
    "model = BuildModel(ModelName=model_name)\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "STEPS_PER_EPOCH = len(y_train)//BATCH_SIZE\r\n",
    "VALIDATION_STEPS = len(y_val)//BATCH_SIZE\r\n",
    "LEARNING_RATE = 0.0002\r\n",
    "NB_EPOCH = 1000 \r\n",
    "MIN_LR = 1e-08\r\n",
    "\r\n",
    "history = ModelDeployment(model = model, \r\n",
    "                          LearningRate = LEARNING_RATE, \r\n",
    "                          ModelName = model_name,\r\n",
    "                          NbOfEpoch = NB_EPOCH,\r\n",
    "                          StepsPerEpoch = STEPS_PER_EPOCH,\r\n",
    "                          ValidationSteps = STEPS_PER_EPOCH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"InceptionResNet.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#save history\r\n",
    "his = pd.DataFrame({'loss': list(history.history[\"loss\"]), \r\n",
    "                    'accuracy': list(history.history[\"accuracy\"]), \r\n",
    "                    'val_loss': list(history.history[\"val_loss\"]), \r\n",
    "                    \"val_accuracy\": list(history.history[\"val_accuracy\"\r\n",
    "                    ])})\r\n",
    "his.to_csv('history.csv', index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HistoryPlotting(history)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# re-evaluate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = model.evaluate(X, Y)\r\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = model.evaluate(X_val, y_val, batch_size=128)\r\n",
    "print(\"test loss, test acc:\", results)\r\n",
    "y_pred = model.predict(X_val)\r\n",
    "classification_report(y_val.argmax(axis=1), y_pred.argmax(axis=1), target_names=labels_list)\r\n",
    "\r\n",
    "print(confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1)))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}