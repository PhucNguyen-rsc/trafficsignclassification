{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import libraries\r\n",
    "import numpy as np\r\n",
    "from PIL import ImageFile\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from tqdm import tqdm\r\n",
    "from tensorflow.keras.applications import *\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "from tensorflow.keras import*\r\n",
    "from tensorflow.keras.models import *\r\n",
    "import numpy as np\r\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from tensorflow.keras import Model, Input\r\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from keras.preprocessing import image\r\n",
    "\r\n",
    "import glob\r\n",
    "from tqdm import tqdm\r\n",
    "import warnings;\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# download dataset\r\n",
    "! wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\r\n",
    "# extract dataset\r\n",
    "! tar -xvzf lfw.tgz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#capture paths to images\r\n",
    "face_images = glob.glob('lfw/**/*.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load all images\r\n",
    "all_images = []\r\n",
    "\r\n",
    "for i in tqdm(face_images):\r\n",
    "  img = image.load_img(i, target_size=(80,80,3))\r\n",
    "  img = image.img_to_array(img)\r\n",
    "  img = img/255.\r\n",
    "  all_images.append(img)\r\n",
    "\r\n",
    "all_images = np.array(all_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# split data into train and validation data\r\n",
    "train_x, val_x = train_test_split(all_images, random_state=42, test_size=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pixalate_image(image, scale_percent = 40):\r\n",
    "  width = int(image.shape[1] * scale_percent / 100)\r\n",
    "  height = int(image.shape[0] * scale_percent / 100)\r\n",
    "  dim = (width, height)\r\n",
    "\r\n",
    "  small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\r\n",
    "  \r\n",
    "  # scale back to original size\r\n",
    "  width = int(small_image.shape[1] * 100 / scale_percent)\r\n",
    "  height = int(small_image.shape[0] * 100 / scale_percent)\r\n",
    "  dim = (width, height)\r\n",
    "\r\n",
    "  low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\r\n",
    "\r\n",
    "  return low_res_image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get low resolution images for the training set\r\n",
    "train_x_px = []\r\n",
    "\r\n",
    "for i in range(train_x.shape[0]):\r\n",
    "  temp = pixalate_image(train_x[i,:,:,:])\r\n",
    "  train_x_px.append(temp)\r\n",
    "\r\n",
    "train_x_px = np.array(train_x_px)\r\n",
    "\r\n",
    "# get low resolution images for the validation set\r\n",
    "val_x_px = []\r\n",
    "\r\n",
    "for i in range(val_x.shape[0]):\r\n",
    "  temp = pixalate_image(val_x[i,:,:,:])\r\n",
    "  val_x_px.append(temp)\r\n",
    "\r\n",
    "val_x_px = np.array(val_x_px)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Input_img = Input(shape=(80, 80, 3))  \r\n",
    "    \r\n",
    "#encoding architecture\r\n",
    "x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(Input_img)\r\n",
    "x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\r\n",
    "x2 = MaxPool2D((2, 2))(x2)\r\n",
    "encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\r\n",
    "\r\n",
    "# decoding architecture\r\n",
    "x3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\r\n",
    "x3 = UpSampling2D((2, 2))(x3)\r\n",
    "x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\r\n",
    "x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\r\n",
    "decoded = Conv2D(3, (3, 3), padding='same')(x1)\r\n",
    "\r\n",
    "autoencoder = Model(Input_img, decoded)\r\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoencoder.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=1, mode='auto')\r\n",
    "\r\n",
    "a_e = autoencoder.fit(train_x_px, train_x,\r\n",
    "                      epochs=50,\r\n",
    "                      batch_size=256,\r\n",
    "                      shuffle=True,\r\n",
    "                      validation_data=(val_x_px, val_x),\r\n",
    "                      callbacks=[early_stopper])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoencoder.save('autoencoder.h5')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}