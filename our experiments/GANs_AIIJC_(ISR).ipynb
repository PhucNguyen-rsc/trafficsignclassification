{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "GANs_AIIJC (ISR)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train upper_base + Evaluation"
      ],
      "metadata": {
        "id": "0q4PtCeSjWfi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q kaggle"
      ],
      "outputs": [],
      "metadata": {
        "id": "9nQZhLdHccHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pip install 'h5py==2.10.0' --force-reinstall"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting six\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Using cached numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ],
      "metadata": {
        "id": "Dh8TVSPlfPt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23998ab3-9c53-4a51-9db6-ca13b1123439"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install ISR"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ISR in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: pyaml in /usr/local/lib/python3.7/dist-packages (from ISR) (21.8.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ISR) (1.19.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from ISR) (2.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ISR) (4.62.0)\n",
            "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.7/dist-packages (from ISR) (2.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Using cached gast-0.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (1.39.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Using cached tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (0.37.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (1.12.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Using cached tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->ISR) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0->ISR) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (1.34.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (0.4.5)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0->ISR) (1.5.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->ISR) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->ISR) (3.7.4.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml->ISR) (3.13)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow-gpu 2.6.0 requires gast==0.4.0, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow-gpu 2.6.0 requires tensorboard~=2.6, but you have tensorboard 2.0.2 which is incompatible.\n",
            "tensorflow-gpu 2.6.0 requires tensorflow-estimator~=2.6, but you have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.2 tensorflow-estimator-2.0.1\n"
          ]
        }
      ],
      "metadata": {
        "id": "7g2ghuL3reD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb36780e-10bc-43ea-cc77-c9abdd2869cd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install --upgrade tensorflow-gpu\r\n",
        "!pip install --upgrade keras"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (5.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.39.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.37.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Collecting gast==0.4.0\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Collecting tensorflow-estimator~=2.6\n",
            "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "Collecting tensorboard~=2.6\n",
            "  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.6.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (0.4.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu) (1.34.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu) (3.5.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.0.1\n",
            "    Uninstalling tensorflow-estimator-2.0.1:\n",
            "      Successfully uninstalled tensorflow-estimator-2.0.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.0.2\n",
            "    Uninstalling tensorboard-2.0.2:\n",
            "      Successfully uninstalled tensorboard-2.0.2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.0.0 requires gast==0.2.2, but you have gast 0.4.0 which is incompatible.\n",
            "tensorflow 2.0.0 requires tensorboard<2.1.0,>=2.0.0, but you have tensorboard 2.6.0 which is incompatible.\n",
            "tensorflow 2.0.0 requires tensorflow-estimator<2.1.0,>=2.0.0, but you have tensorflow-estimator 2.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.4.0 tensorboard-2.6.0 tensorflow-estimator-2.6.0\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
          ]
        }
      ],
      "metadata": {
        "id": "mmFfXM0BboT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60798ff-5f16-4e27-86ca-a080b75c8e47"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!mkdir ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "outputs": [],
      "metadata": {
        "id": "m6bUlav_sc4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!kaggle datasets download -d congthjnh/aiijc-new128x128"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading aiijc-new128x128.zip to /content\n",
            " 99% 918M/930M [00:21<00:00, 41.6MB/s]\n",
            "100% 930M/930M [00:21<00:00, 46.2MB/s]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhcHPBK5ZhDw",
        "outputId": "4b5dd94e-f754-4bf7-bdf1-c4cb44ed8774"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip -qq \"/content/aiijc-new128x128.zip\" -d \"/content/aiijc-new128x128 (train)\""
      ],
      "outputs": [],
      "metadata": {
        "id": "458D5A2ej5-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython.display import Image, display\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import tensorflow\r\n",
        "import math\r\n",
        "import random\r\n",
        "from PIL import ImageFile, Image\r\n",
        "import pandas as pd\r\n",
        "from tensorflow.keras.models import *\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.preprocessing.image import *\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "from tqdm import tqdm\r\n",
        "import pickle\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.applications import *\r\n",
        "from tensorflow.keras import *\r\n",
        "import tensorflow as tf \r\n",
        "import cv2\r\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n",
        "%matplotlib inline\r\n",
        "from shutil import copyfile\r\n",
        "import glob\r\n",
        "import imageio\r\n",
        "import PIL\r\n",
        "import time"
      ],
      "outputs": [],
      "metadata": {
        "id": "VXTAuB2kdKOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import glob\r\n",
        "import imageio\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import PIL\r\n",
        "from tensorflow.keras import layers\r\n",
        "import time\r\n",
        "\r\n",
        "from IPython import display"
      ],
      "outputs": [],
      "metadata": {
        "id": "bJUEqF--j_JD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "root_dir = \"/content/aiijc-new128x128 (train)/New folder\"\r\n",
        "class_names = os.listdir(root_dir)\r\n",
        "class_names.sort()\r\n",
        "class_names.pop()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unknown'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "id": "HozjpXamkCYy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6bd554d-0980-4979-9fe4-0d5cc7fc701b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "x = []\r\n",
        "y = []\r\n",
        "z = []\r\n",
        "X_train_paths = []\r\n",
        "X_val_paths = []\r\n",
        "y_val = []\r\n",
        "\r\n",
        "for label in tqdm(class_names):\r\n",
        "  img_names = os.listdir(root_dir + \"/\" + label)\r\n",
        "  count = 0\r\n",
        "  while count < 5:\r\n",
        "    x.append(os.path.join(label, img_names[count]))\r\n",
        "    y.append(\"sign\")\r\n",
        "    X_val_paths.append(os.path.join(label, img_names[count]))\r\n",
        "    y_val.append(1)\r\n",
        "    z.append(\"val\")\r\n",
        "    img_names.pop(0)\r\n",
        "    count += 1\r\n",
        "  for img_name in img_names:\r\n",
        "    x.append(os.path.join(label, img_name))\r\n",
        "    y.append(\"sign\")\r\n",
        "    X_train_paths.append(os.path.join(os.path.join(label, img_name)))\r\n",
        "    z.append(\"train\")\r\n",
        "\r\n",
        "unknown_class = os.listdir(root_dir + \"/\" + \"unknown\")\r\n",
        "for i in tqdm(range(len(unknown_class))):\r\n",
        "  x.append(os.path.join(\"unknown\", unknown_class[i]))\r\n",
        "  y.append(\"unknown\")\r\n",
        "  X_val_paths.append(os.path.join(\"unknown\", unknown_class[i]))\r\n",
        "  y_val.append(0)\r\n",
        "  z.append(\"val\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 181/181 [00:00<00:00, 734.40it/s]\n",
            "100%|██████████| 971/971 [00:00<00:00, 250194.69it/s]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKkNVP1kZ7RM",
        "outputId": "576feea0-ab12-4d42-a38d-82798bb77cf6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "image_size = 64\r\n",
        "\r\n",
        "def read_and_prep_images(img_paths, img_height = image_size, img_width = image_size):\r\n",
        "    imgs = [load_img(os.path.join(root_dir,img_path), target_size = (img_height, img_width)) for img_path in tqdm(img_paths)]\r\n",
        "    img_array = np.array([img_to_array(img) for img in imgs])\r\n",
        "    output = img_array/255.0\r\n",
        "    return(output)\r\n",
        "\r\n",
        "X_train = read_and_prep_images(X_train_paths)\r\n",
        "np.random.shuffle(X_train)\r\n",
        "X_test = read_and_prep_images(X_val_paths)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45482/45482 [00:40<00:00, 1111.29it/s]\n",
            "100%|██████████| 1876/1876 [00:01<00:00, 1191.70it/s]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiGd6OWRZ-Ni",
        "outputId": "b5cf11ac-3570-4490-fef3-d529b5b49047"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "BUFFER_SIZE = 45000\r\n",
        "BATCH_SIZE = 64\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mL-lNZjLaBC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ISR.models import RRDN\r\n",
        "from ISR.models import Discriminator\r\n",
        "from ISR.models import Cut_VGG19\r\n",
        "\r\n",
        "lr_train_patch_size = 32\r\n",
        "layers_to_extract = [5, 9]\r\n",
        "scale = 2\r\n",
        "hr_train_patch_size = lr_train_patch_size * scale\r\n",
        "\r\n",
        "rrdn  = RRDN(arch_params = {'C':4, 'D':3, 'G':64, 'G0':64, 'T':10, 'x':scale}, patch_size=lr_train_patch_size)\r\n",
        "f_ext = Cut_VGG19(patch_size = hr_train_patch_size, layers_to_extract = layers_to_extract)\r\n",
        "discr = Discriminator(patch_size = hr_train_patch_size, kernel_size = 3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "metadata": {
        "id": "Fxza6fV-aE0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ac5ee8-8ea4-4c45-fc92-f9bcb5dce65a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rrdn.model.load_weights('/content/drive/MyDrive/Geoservice analysis/ISR/generator(4).hdf5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "mAKyFghGaHqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "discr.model.load_weights('/content/drive/MyDrive/Geoservice analysis/ISR/discriminator(4).hdf5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "iUV3_M48anMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gen_base = load_model('/content/drive/MyDrive/Geoservice analysis/ISR/gen_base.h5')\r\n",
        "discr_base = load_model('/content/drive/MyDrive/Geoservice analysis/ISR/discr_base.h5')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9oh8Vima7W0",
        "outputId": "f9fbc5f1-20c0-431a-8d1b-2446458de797"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rrdn.model.trainable = False\r\n",
        "\r\n",
        "generator = Sequential([\r\n",
        "    gen_base,\r\n",
        "    rrdn.model                   \r\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "P-j78UwybVl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "discr.model.trainable = False\r\n",
        "\r\n",
        "discriminator = Sequential([\r\n",
        "    discr.model,\r\n",
        "    discr_base              \r\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "x_ih2YowdG2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "A1lSV_CudIIn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def discriminator_loss(real_output, fake_output):\r\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\r\n",
        "    total_loss = real_loss + fake_loss\r\n",
        "    return total_loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "iSlA20Z6dOPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def generator_loss(fake_output):\r\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-K0wdRC2dQnj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\r\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KzApnUSIdS8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "EPOCHS = 50\r\n",
        "noise_dim = 100\r\n",
        "num_examples_to_generate = 64\r\n",
        "\r\n",
        "# You will reuse this seed overtime (so it's easier)\r\n",
        "# to visualize progress in the animated GIF)\r\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "outputs": [],
      "metadata": {
        "id": "9_KT1YCXdVm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "@tf.function #converts regular python code to a callable Tensorflow graph function\r\n",
        "def train_step(images):\r\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim]) #noise dim is the latent representation\r\n",
        "    print('Done noise')\r\n",
        "\r\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: #one for generator, the other for discriminator\r\n",
        "      generated_images = generator(noise, training = True) #generate a batch of fake images (from noise)\r\n",
        "      print('Done generated images')\r\n",
        "\r\n",
        "      real_output = discriminator(images, training = True)\r\n",
        "      fake_output = discriminator(generated_images, training = True)\r\n",
        "      print('Done fake output')\r\n",
        "\r\n",
        "      gen_loss = generator_loss(fake_output)\r\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\r\n",
        "\r\n",
        "      print('Done loss')\r\n",
        "\r\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n",
        "    #gen loss is target, generator is the source (gradient tape will record all of the operatirons happen to peform backward gradient)\r\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n",
        "\r\n",
        "    print('Done gradients')\r\n",
        "\r\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\r\n",
        "\r\n",
        "    print('Done opt')"
      ],
      "outputs": [],
      "metadata": {
        "id": "jfR7XQ7bdYR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train(dataset, epochs):\r\n",
        "  for epoch in range(epochs):\r\n",
        "    start = time.time()\r\n",
        "\r\n",
        "    print('Start-timing')\r\n",
        "\r\n",
        "    for image_batch in dataset:\r\n",
        "      train_step(image_batch)\r\n",
        "      print('1 step')\r\n",
        "    \r\n",
        "    print('---Done 1 epoch---')\r\n",
        "\r\n",
        "    # Produce images for the GIF as you go\r\n",
        "    # Save the model every 1 epochs\r\n",
        "    gen_base.save('/content/drive/MyDrive/Geoservice analysis/gen_base.h5')\r\n",
        "    discr_base.save('/content/drive/MyDrive/Geoservice analysis/discr_base.h5')\r\n",
        "    generator.save('/content/drive/MyDrive/Geoservice analysis/big_generator.h5')\r\n",
        "    discriminator.save('/content/drive/MyDrive/Geoservice analysis/big_discriminator.h5')\r\n",
        "\r\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\r\n",
        "\r\n",
        "      # Generate after the final epoch\r\n",
        "  display.clear_output(wait = True)\r\n",
        "  generate_and_save_images(generator,\r\n",
        "                           epochs,\r\n",
        "                           seed\r\n",
        "                           )"
      ],
      "outputs": [],
      "metadata": {
        "id": "dhlx2zcaddns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\r\n",
        "  # Notice `training` is set to False.\r\n",
        "  # This is so all layers run in inference mode (batchnorm).\r\n",
        "  predictions = model(test_input, training=False)\r\n",
        "\r\n",
        "  fig = plt.figure(figsize = (8, 8))\r\n",
        "\r\n",
        "  for i in range(predictions.shape[0]):\r\n",
        "      plt.subplot(8, 8, i + 1)\r\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\r\n",
        "      plt.axis('off')\r\n",
        "\r\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\r\n",
        "  plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "lGQx2le2lwUl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train(train_dataset, 5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start-timing\n",
            "Done noise\n",
            "Done generated images\n",
            "Done fake output\n",
            "Done loss\n",
            "Done gradients\n",
            "Done opt\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n",
            "1 step\n"
          ]
        }
      ],
      "metadata": {
        "id": "F4tR8yN2djg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d43e400-1284-476a-ea15-115ab9bdf57a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "discr_base_val = load_model('/content/drive/MyDrive/Geoservice analysis/discr_base.h5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jf4aNcESdlOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "discriminator_val = Sequential([\r\n",
        "    discr.model,\r\n",
        "    discr_base_val  \r\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "3bO622ADdrQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import sklearn\r\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "outputs": [],
      "metadata": {
        "id": "YWX9gb1ndtbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred= discriminator_val.predict(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AQP1HxdMdwCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(roc_auc_score(y_val, y_pred))"
      ],
      "outputs": [],
      "metadata": {
        "id": "2BdNvs89dyeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ISR base"
      ],
      "metadata": {
        "id": "1ZnkQCb-jTBd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!kaggle datasets download -d congthjnh/encoded-imgs"
      ],
      "outputs": [],
      "metadata": {
        "id": "IuGpRPN8eb-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip -qq \"/content/encoded-imgs.zip\" -d \"/content/aiijc-80x80 (train)\""
      ],
      "outputs": [],
      "metadata": {
        "id": "qo1m7I_Psm-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.read_csv('/content/train.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "aEjllL-hQA8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_uk = df.loc[df['label']=='unknown']"
      ],
      "outputs": [],
      "metadata": {
        "id": "8HRtSWEdIQfk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in tqdm(df_uk['filename']):\r\n",
        "  df_uk.loc[df['filename']==i, 'filename']= i.replace('train_images/','')"
      ],
      "outputs": [],
      "metadata": {
        "id": "2CkGxKJIZMtq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in tqdm(df_uk['filename']):\r\n",
        "  os.remove('/content/aiijc-80x80 (train)/Autoencoder/'+i)"
      ],
      "outputs": [],
      "metadata": {
        "id": "k0S_5gm1ifbw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.makedirs('aiijc-80x80 (validation)', exist_ok = True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "i1j_XtBRg2Lt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in tqdm(random.sample(os.listdir('/content/aiijc-80x80 (train)/Autoencoder'), k = math.floor(len(os.listdir('/content/aiijc-80x80 (train)/Autoencoder'))/30))):\r\n",
        "  copyfile('/content/aiijc-80x80 (train)/Autoencoder/' + i, '/content/aiijc-80x80 (validation)/' + i)\r\n",
        "  os.remove('/content/aiijc-80x80 (train)/Autoencoder/' + i)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "p3OA_LrpgE6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def preprocess_img_init(item, name):\r\n",
        "  if name == 'train':\r\n",
        "    img = load_img(f'/content/aiijc-80x80 ({name})/Autoencoder/' + item)\r\n",
        "    img_array = img_to_array(img)\r\n",
        "    img_resized = tf.image.resize(img_array, size = [32, 32], preserve_aspect_ratio = True)\r\n",
        "    save_img(f'/content/aiijc-32x32 ({name})/' + item, x = img_resized)\r\n",
        "  elif name == 'validation':\r\n",
        "    img = load_img(f'/content/aiijc-80x80 ({name})/' + item)\r\n",
        "    img_array = img_to_array(img)\r\n",
        "    img_resized = tf.image.resize(img_array, size = [32, 32], preserve_aspect_ratio = True)\r\n",
        "    save_img(path = f'/content/aiijc-32x32 ({name})/' + item, x = img_resized)   "
      ],
      "outputs": [],
      "metadata": {
        "id": "5nNUvBWZ2Kas"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.makedirs('aiijc-32x32 (validation)', exist_ok = True) \r\n",
        "os.makedirs('aiijc-32x32 (train)', exist_ok = True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hp1iDINI2Xff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in tqdm(os.listdir('/content/aiijc-80x80 (train)/Autoencoder')):\r\n",
        "  preprocess_img_init(item = i, name = 'train')\r\n",
        "\r\n",
        "for i in tqdm(os.listdir('/content/aiijc-80x80 (validation)')):\r\n",
        "  preprocess_img_init(item = i, name = 'validation')"
      ],
      "outputs": [],
      "metadata": {
        "id": "d3b-cQIf2mho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.makedirs('/content/aiijc-64x64 (train)', exist_ok = True)\r\n",
        "os.makedirs('/content/aiijc-64x64 (validation)', exist_ok = True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jrc86PWEoYiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def preprocess_img(item, name):\r\n",
        "  if name =='train':\r\n",
        "    img = load_img(f'/content/aiijc-32x32 ({name})/' + item)\r\n",
        "    img_array = img_to_array(img)\r\n",
        "    img_resized = tf.image.resize(img_array, size = [64, 64], preserve_aspect_ratio = True)\r\n",
        "    save_img(f'/content/aiijc-64x64 ({name})/' + item, x = img_resized)\r\n",
        "  elif name =='validation':\r\n",
        "    img = load_img(f'/content/aiijc-32x32 ({name})/' + item)\r\n",
        "    img_array = img_to_array(img)\r\n",
        "    img_resized = tf.image.resize(img_array, size = [64, 64], preserve_aspect_ratio = True)\r\n",
        "    save_img(path = f'/content/aiijc-64x64 ({name})/' + item, x = img_resized)    "
      ],
      "outputs": [],
      "metadata": {
        "id": "ihyg-PmAmH7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in tqdm(os.listdir('/content/aiijc-32x32 (train)')):\r\n",
        "  preprocess_img(item = i, name = 'train')"
      ],
      "outputs": [],
      "metadata": {
        "id": "4SfG4QxTo9zg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in tqdm(os.listdir('/content/aiijc-32x32 (validation)')):\r\n",
        "  preprocess_img(item = i, name = 'validation')"
      ],
      "outputs": [],
      "metadata": {
        "id": "-Jo_QEw2qB3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ISR.models import RRDN\r\n",
        "from ISR.models import Discriminator\r\n",
        "from ISR.models import Cut_VGG19\r\n",
        "\r\n",
        "lr_train_patch_size = 32\r\n",
        "layers_to_extract = [5, 9]\r\n",
        "scale = 2\r\n",
        "hr_train_patch_size = lr_train_patch_size * scale\r\n",
        "\r\n",
        "rrdn  = RRDN(arch_params = {'C':4, 'D':3, 'G':64, 'G0':64, 'T':10, 'x':scale}, patch_size = lr_train_patch_size)\r\n",
        "f_ext = Cut_VGG19(patch_size = hr_train_patch_size, layers_to_extract = layers_to_extract)\r\n",
        "discr = Discriminator(patch_size = hr_train_patch_size, kernel_size = 3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "R7q4ORn9fDCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from shutil import rmtree\r\n",
        "\r\n",
        "rmtree('aiijc-80x80 (train)')\r\n",
        "rmtree('aiijc-80x80 (validation)')"
      ],
      "outputs": [],
      "metadata": {
        "id": "hAwrmF7a3mYC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ISR.train import Trainer\r\n",
        "loss_weights = {\r\n",
        "  'generator': 0.0,\r\n",
        "  'feature_extractor': 0.0833,\r\n",
        "  'discriminator': 0.01\r\n",
        "}\r\n",
        "losses = {\r\n",
        "  'generator': 'mae',\r\n",
        "  'feature_extractor': 'mse',\r\n",
        "  'discriminator': 'binary_crossentropy'\r\n",
        "}\r\n",
        "\r\n",
        "log_dirs = {'logs': './logs', 'weights': './weights'}\r\n",
        "\r\n",
        "learning_rate = {'initial_value': 0.0004, 'decay_factor': 0.5, 'decay_frequency': 3}\r\n",
        "\r\n",
        "flatness = {'min': 0.0, 'max': 0.15, 'increase': 0.01, 'increase_frequency': 5}\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    generator = rrdn,\r\n",
        "    discriminator = discr,\r\n",
        "    feature_extractor = f_ext,\r\n",
        "    lr_train_dir = '/content/aiijc-32x32 (train)',\r\n",
        "    hr_train_dir = '/content/aiijc-64x64 (train)',\r\n",
        "    lr_valid_dir = '/content/aiijc-32x32 (validation)',\r\n",
        "    hr_valid_dir = '/content/aiijc-64x64 (validation)',\r\n",
        "    loss_weights = loss_weights,\r\n",
        "    learning_rate = learning_rate,\r\n",
        "    flatness = flatness,\r\n",
        "    dataname = 'image_dataset',\r\n",
        "    log_dirs = log_dirs,\r\n",
        "    weights_generator = '/content/drive/MyDrive/Geoservice analysis/ISR/generator(3).hdf5',\r\n",
        "    weights_discriminator = '/content/drive/MyDrive/Geoservice analysis/ISR/discriminator(3).hdf5',\r\n",
        "    n_validation = 100,\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "zpnqcEb8xerA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer.train(\r\n",
        "    epochs = 7,\r\n",
        "    steps_per_epoch = 1220,\r\n",
        "    batch_size = 38,\r\n",
        "    monitored_metrics = {'val_PSNR_Y': 'max'}\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "IlAjyLetznYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gen_model = rrdn.model\r\n",
        "gen_model.save('/content/drive/MyDrive/Geoservice analysis/ISR/generator(4).hdf5')\r\n",
        "discr_model = discr.model\r\n",
        "discr_model.save('/content/drive/MyDrive/Geoservice analysis/ISR/discriminator(4).hdf5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "TmQSBYd5nG_6"
      }
    }
  ]
}