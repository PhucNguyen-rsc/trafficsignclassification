{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import os\r\n",
    "import tensorflow as tf\r\n",
    "from PIL import ImageFile\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from tensorflow.python.keras.models import Sequential\r\n",
    "from tensorflow.python.keras.layers import *\r\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\r\n",
    "from tensorflow.keras.backend import *\r\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.ensemble import IsolationForest\r\n",
    "from sklearn import svm\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.applications import *\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "from tensorflow.keras import*\r\n",
    "from tensorflow.keras.models import *\r\n",
    "import tensorflow as tf \r\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One-class svm and Isolation Forest to classify unknown class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_dir = \"../input/aiijc-new128x128/New folder\"\r\n",
    "class_names = os.listdir(root_dir)\r\n",
    "class_names.sort()\r\n",
    "class_names.pop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create dataset with known/unknown\r\n",
    "x = []\r\n",
    "y = []\r\n",
    "z = []\r\n",
    "X_train_paths = []\r\n",
    "X_val_paths = []\r\n",
    "y_val = []\r\n",
    "\r\n",
    "for label in tqdm(class_names):\r\n",
    "  img_names = os.listdir(root_dir + \"/\" + label)\r\n",
    "  count = 0\r\n",
    "  while count < 5:\r\n",
    "    x.append(os.path.join(label, img_names[count]))\r\n",
    "    y.append(\"sign\")\r\n",
    "    X_val_paths.append(os.path.join(label, img_names[count]))\r\n",
    "    y_val.append(1)\r\n",
    "    z.append(\"val\")\r\n",
    "    img_names.pop(0)\r\n",
    "    count+=1\r\n",
    "  for img_name in img_names:\r\n",
    "    x.append(os.path.join(label, img_name))\r\n",
    "    y.append(\"sign\")\r\n",
    "    X_train_paths.append(os.path.join(os.path.join(label, img_name)))\r\n",
    "    z.append(\"train\")\r\n",
    "\r\n",
    "unknown_class = os.listdir(root_dir + \"/\" + \"unknown\")\r\n",
    "for i in tqdm(range(len(unknown_class))):\r\n",
    "  x.append(os.path.join(\"unknown\", unknown_class[i]))\r\n",
    "  y.append(\"unknown\")\r\n",
    "  X_val_paths.append(os.path.join(\"unknown\", unknown_class[i]))\r\n",
    "  y_val.append(0)\r\n",
    "  z.append(\"val\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame({\"filename\": x, \"label\": y, \"\": z})\r\n",
    "df.tail(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(X_train_paths), len(X_val_paths))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_size = 32\r\n",
    "\r\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\r\n",
    "    imgs = [load_img(os.path.join(root_dir,img_path), target_size=(img_height, img_width)) for img_path in tqdm(img_paths)]\r\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\r\n",
    "    #output = img_array\r\n",
    "    output = tf.keras.applications.resnet.preprocess_input(img_array)\r\n",
    "    return(output)\r\n",
    "\r\n",
    "X_train = read_and_prep_images(X_train_paths)\r\n",
    "X_val = read_and_prep_images(X_val_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# using resnet for extracting features\r\n",
    "resnet_model = tf.keras.applications.ResNet50(input_shape=(image_size, image_size, 3), \r\n",
    "                                              weights=\"imagenet\", include_top=False, \r\n",
    "                                              pooling='avg')\r\n",
    "                              \r\n",
    "X_train = resnet_model.predict(X_train)\r\n",
    "X_val = resnet_model.predict(X_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ss = StandardScaler()\r\n",
    "ss.fit(X_train)\r\n",
    "X_train = ss.transform(X_train)\r\n",
    "X_val = ss.transform(X_val)\r\n",
    "\r\n",
    "# Take PCA to reduce feature space dimensionality\r\n",
    "pca = PCA(n_components=512, whiten=True)\r\n",
    "pca = pca.fit(X_train)\r\n",
    "print('Explained variance percentage = %0.2f' % sum(pca.explained_variance_ratio_))\r\n",
    "X_train = pca.transform(X_train)\r\n",
    "X_val = pca.transform(X_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test one-class svm and isolation forest\r\n",
    "oc_svm_clf = svm.OneClassSVM(gamma=0.1, kernel='rbf', nu = 0.01)  \r\n",
    "if_clf = IsolationForest(contamination=0.1, max_features=1.0, max_samples=1.0, n_estimators=40)  \r\n",
    "\r\n",
    "oc_svm_clf.fit(X_train)\r\n",
    "if_clf.fit(X_train)\r\n",
    "\r\n",
    "oc_svm_preds = oc_svm_clf.predict(X_val)\r\n",
    "if_preds = if_clf.predict(X_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df = pd.DataFrame({\"filename\": X_val_paths, \"label\": y_val})\r\n",
    "test_df.tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svm_if_results = pd.DataFrame({\r\n",
    "  'filename': X_val_paths,\r\n",
    "  'oc_svm_preds': [0 if x == -1 else 1 for x in oc_svm_preds],\r\n",
    "  'if_preds': [0 if x == -1 else 1 for x in if_preds]\r\n",
    "})\r\n",
    "\r\n",
    "svm_if_results = svm_if_results.merge(test_df)\r\n",
    "svm_if_results.tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('roc auc score: if_preds')\r\n",
    "if_preds = svm_if_results['if_preds']\r\n",
    "actual = svm_if_results['label']\r\n",
    "print(roc_auc_score(actual, if_preds))\r\n",
    "print(classification_report(actual, if_preds))\r\n",
    "sns.heatmap(confusion_matrix(actual, if_preds), annot=True , fmt='2.0f')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('roc auc score: oc_svm_preds')\r\n",
    "oc_svm_preds=svm_if_results['oc_svm_preds']\r\n",
    "actual=svm_if_results['label']\r\n",
    "print(roc_auc_score(actual, oc_svm_preds))\r\n",
    "print(classification_report(actual, oc_svm_preds))\r\n",
    "sns.heatmap(confusion_matrix(actual, oc_svm_preds),annot=True,fmt='2.0f')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid-search based algorithms to find best hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#grid search for ocsvm\r\n",
    "clf = svm.OneClassSVM()\r\n",
    "gammas = np.linspace(0.001, 0.01, 100)\r\n",
    "nus = np.linspace(0.070, 0.1, 3)\r\n",
    "count = 0\r\n",
    "results = []\r\n",
    "paras = []\r\n",
    "for gamma in gammas:\r\n",
    "    for nu in nus:\r\n",
    "        clf.set_params(gamma=gamma, nu=nu)\r\n",
    "        clf.fit(X_train)\r\n",
    "        oc_svm_preds = clf.predict(X_val)\r\n",
    "        svm_if_results = pd.DataFrame({\r\n",
    "        'filename': X_val_paths,\r\n",
    "        'oc_svm_preds': [0 if x == -1 else 1 for x in oc_svm_preds]\r\n",
    "        })\r\n",
    "        svm_if_results = svm_if_results.merge(test_df)\r\n",
    "        \r\n",
    "        count += 1\r\n",
    "        print(\"VERSION {}\".format(count))\r\n",
    "        #print('roc auc score: oc_svm_preds (gamma = {}, nu = {})'.format(gamma, nu))\r\n",
    "        paras.append((gamma, nu))\r\n",
    "        oc_svm_preds = svm_if_results['oc_svm_preds']\r\n",
    "        actual = svm_if_results['label']\r\n",
    "        score = roc_auc_score(actual, oc_svm_preds)\r\n",
    "        print(score)\r\n",
    "        results.append(score)\r\n",
    "        #print(classification_report(actual, oc_svm_preds))\r\n",
    "        #sns.heatmap(confusion_matrix(actual, oc_svm_preds),annot=True,fmt='2.0f')\r\n",
    "        #plt.show()\r\n",
    "\r\n",
    "max_acc = max(results)\r\n",
    "print(paras[results.index(max_acc)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#contamination=0.1, max_features=1.0, max_samples=1.0\r\n",
    "contaminations = [0.1,0.01,0.001,0.0001]\r\n",
    "max_features = [1,2,3]\r\n",
    "max_samples = [1, 10000]\r\n",
    "n_estimators = [20, 40, 60, 80, 100]\r\n",
    "count = 0\r\n",
    "results = []\r\n",
    "paras = []\r\n",
    "if_clf = IsolationForest()\r\n",
    "for contamination in contaminations:\r\n",
    "    for max_feature in max_features:\r\n",
    "        for max_sample in max_samples:\r\n",
    "            for n_estimator in n_estimators:\r\n",
    "                if_clf.set_params(contamination=contamination, max_features=max_feature, max_samples=max_sample, n_estimators = n_estimator)\r\n",
    "                if_clf.fit(X_train)\r\n",
    "                if_preds = if_clf.predict(X_val)\r\n",
    "                svm_if_results = pd.DataFrame({\r\n",
    "            'filename': X_val_paths,\r\n",
    "                'if_preds': [0 if x == -1 else 1 for x in if_preds]\r\n",
    "                })\r\n",
    "                svm_if_results = svm_if_results.merge(test_df)\r\n",
    "            \r\n",
    "                count += 1\r\n",
    "                print(\"VERSION {}\".format(count))\r\n",
    "                print('roc auc score: oc_svm_preds (contamination = {}, max_feature = {}, max_sample = {})'.format(contamination, max_feature, max_sample))\r\n",
    "                paras.append((contamination, max_feature, max_sample))\r\n",
    "                if_preds = svm_if_results['if_preds']\r\n",
    "                actual = svm_if_results['label']\r\n",
    "                score = roc_auc_score(actual, if_preds)\r\n",
    "                print(score)\r\n",
    "                results.append(score)\r\n",
    "                print(classification_report(actual, oc_svm_preds))\r\n",
    "                sns.heatmap(confusion_matrix(actual, oc_svm_preds),annot=True,fmt='2.0f')\r\n",
    "                plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using auto-encoder to classify unknown"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# VGG16 based model\r\n",
    "encoder = Sequential([Input(shape=(32,32,3)),\r\n",
    "                      \r\n",
    "                      Conv2D(64, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(64, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      MaxPooling2D(),\r\n",
    "                      \r\n",
    "                      Conv2D(128, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(128, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      MaxPooling2D(),\r\n",
    "                      \r\n",
    "                      Conv2D(256, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(256, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(256, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      MaxPooling2D(),\r\n",
    "                  \r\n",
    "                      Conv2D(512, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(512, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(512, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      MaxPooling2D(),\r\n",
    "                      \r\n",
    "                      Conv2D(512, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(512, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      Conv2D(512, (3,3), strides=1, padding='same'),\r\n",
    "                      LeakyReLU(0.2),\r\n",
    "                      BatchNormalization(),\r\n",
    "                      \r\n",
    "                      Flatten(),\r\n",
    "                      Dense(1024, activation=LeakyReLU(0.2))\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decoder= Sequential([\r\n",
    "    Input(1024,),\r\n",
    "    Dense(2048),\r\n",
    "    Reshape(target_shape = (2,2,512)),\r\n",
    "    \r\n",
    "    Conv2DTranspose(512,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(512,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(512,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    \r\n",
    "    UpSampling2D(),\r\n",
    "    Conv2DTranspose(512,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(512,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(512,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    \r\n",
    "    UpSampling2D(),\r\n",
    "    Conv2DTranspose(256,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(256,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(256,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    \r\n",
    "    UpSampling2D(),\r\n",
    "    Conv2DTranspose(128,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(128,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "\r\n",
    "    UpSampling2D(),\r\n",
    "    Conv2DTranspose(64,(3,3), strides =1 ,padding='same'),\r\n",
    "    LeakyReLU(0.2),\r\n",
    "    BatchNormalization(),\r\n",
    "    Conv2DTranspose(64,(3,3), strides =1, padding='same'),\r\n",
    "    LeakyReLU(0.2),    \r\n",
    "    BatchNormalization(),\r\n",
    "    \r\n",
    "\r\n",
    "    Conv2DTranspose(3,(3,3), padding ='same'),\r\n",
    "    Activation('sigmoid')\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decoder.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoencoder = Sequential([encoder,\r\n",
    "                          decoder\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clear_session()\r\n",
    "\r\n",
    "autoencoder.compile('adam', loss='binary_crossentropy')\r\n",
    "\r\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('autoencoder_1.h5', monitor ='val_loss',save_best_only = True)\r\n",
    "history = autoencoder.fit(X_train,X_train, \r\n",
    "                          epochs=100, \r\n",
    "                          batch_size=128, \r\n",
    "                          verbose=1,\r\n",
    "                          shuffle=True,\r\n",
    "                          validation_data=(X_val,X_val),\r\n",
    "                          callbacks = [checkpoint])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import keras\r\n",
    "autoencoder =  keras.models.load_model('./autoencoder_1.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decode_imgs = autoencoder.predict(X_val)\r\n",
    "decode_imgs = decode_imgs.reshape(len(X_val),32,32, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\r\n",
    "loss=[]\r\n",
    "for i in tqdm(range(len(X_val))):\r\n",
    "    a=mse(X_val[i], decode_imgs[i]).numpy()\r\n",
    "    loss.append(a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = 10\r\n",
    "plt.figure(figsize=(20, 4))\r\n",
    "for i in range(n):\r\n",
    "    # Display original\r\n",
    "    ax = plt.subplot(2, n, i + 1)\r\n",
    "    plt.imshow(X_val[i].reshape(32, 32,3))\r\n",
    "    plt.title('Original Image')\r\n",
    "    plt.gray()\r\n",
    "    ax.get_xaxis().set_visible(False)\r\n",
    "    ax.get_yaxis().set_visible(False)\r\n",
    "\r\n",
    "    # Display reconstruction\r\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\r\n",
    "    plt.imshow(decode_imgs[i].reshape(32, 32,3))\r\n",
    "    plt.title('Decoded Image')\r\n",
    "    plt.gray()\r\n",
    "    ax.get_xaxis().set_visible(False)\r\n",
    "    ax.get_yaxis().set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}